{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acaf6084",
   "metadata": {},
   "source": [
    "# Jira Satisfaction Analysis (BERT)\n",
    "## Instructions\n",
    "1. **Secrets**: Add JIRA_URL, JIRA_EMAIL, JIRA_API_TOKEN in the sidebar (Key icon).\n",
    "2. **Runtime**: Change Runtime type to GPU (Runtime > Change runtime type > T4 GPU).\n",
    "3. **Run**: Click the Play button below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1318c3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Jira Satisfaction Analysis (All-in-One)\n",
    "# COPY THIS ENTIRE SCRIPT INTO A GOOGLE COLAB CELL\n",
    "# ==============================================================================\n",
    "# 1. SETUP & INSTALLATION\n",
    "# ==============================================================================\n",
    "import os\n",
    "import sys\n",
    "\n",
    "def install_dependencies():\n",
    "    print(\"üöÄ Installing dependencies...\")\n",
    "    os.system('pip install transformers torch pandas requests scikit-learn python-dotenv joblib')\n",
    "    print(\"‚úÖ Dependencies installed.\")\n",
    "\n",
    "# Check if running in Colab to install dependencies automatically\n",
    "if 'google.colab' in sys.modules:\n",
    "    install_dependencies()\n",
    "    from google.colab import drive\n",
    "    from google.colab import userdata\n",
    "    \n",
    "    # Mount Drive\n",
    "    print(\"üìÇ Mounting Google Drive...\")\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Set Paths for Colab\n",
    "    BASE_DIR = '/content/drive/MyDrive/jira_analysis'\n",
    "    os.makedirs(BASE_DIR, exist_ok=True)\n",
    "    DATA_DIR = os.path.join(BASE_DIR, 'data')\n",
    "    MODELS_DIR = os.path.join(BASE_DIR, 'models/bert')\n",
    "    \n",
    "    # Get Secrets (Make sure to add these in Colab Secrets tab)\n",
    "       try:\n",
    "        JIRA_URL = userdata.get('JIRA_URL')\n",
    "        JIRA_EMAIL = userdata.get('JIRA_EMAIL')\n",
    "        JIRA_API_TOKEN = userdata.get('JIRA_API_TOKEN')\n",
    "    except:\n",
    "        print(\"‚ö†Ô∏è Secrets not found! Please add JIRA_URL, JIRA_EMAIL, JIRA_API_TOKEN in Colab Secrets.\")\n",
    "        JIRA_URL = \"https://certifyos.atlassian.net/\"\n",
    "        JIRA_EMAIL = \"anurag.rai@certifyos.com\"\n",
    "        JIRA_API_TOKEN = \"ATATT3xFfGF0BK0DPRifn8dXP3AnH0fS8TKfH-QCU5Ah_h3JxSqxKo0QryD1A0B37gydyLp4QMMPLejMxVbQj9IcsObdIPyF3v8t3y4mTX07XQuCH7B0uBSQFUlkGUMj3IzbUsFx21yBGL-6w0bzXfm2D_XJvsQFBe8a-AF-5epvchUeCMpmAL8=47A52710\"\n",
    "else:\n",
    "\n",
    "    # Local fallback\n",
    "    BASE_DIR = '.'\n",
    "    DATA_DIR = 'data'\n",
    "    MODELS_DIR = 'models/bert'\n",
    "    JIRA_URL = os.getenv(\"JIRA_URL\")\n",
    "    JIRA_EMAIL = os.getenv(\"JIRA_EMAIL\")\n",
    "    JIRA_API_TOKEN = os.getenv(\"JIRA_API_TOKEN\")\n",
    "\n",
    "# Debug Config\n",
    "print(f\"üîß Config Check:\")\n",
    "print(f\"   URL: {JIRA_URL}\")\n",
    "print(f\"   Email: {JIRA_EMAIL}\")\n",
    "print(f\"   Token: {'*' * 5}{JIRA_API_TOKEN[-4:] if JIRA_API_TOKEN else 'None'}\")\n",
    "\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "\n",
    "# ==============================================================================\n",
    "# IMPORT UTILS\n",
    "# ==============================================================================\n",
    "def load_data(file_path):\n",
    "    \"\"\"Load data from CSV or Excel file.\"\"\"\n",
    "    if file_path.endswith('.xlsx') or file_path.endswith('.xls'):\n",
    "        print(f\"üìÑ Detected Excel file. Converting {file_path}...\")\n",
    "        try:\n",
    "            df = pd.read_excel(file_path)\n",
    "            # Save as CSV for consistency in future steps\n",
    "            csv_path = file_path.replace('.xlsx', '.csv').replace('.xls', '.csv')\n",
    "            df.to_csv(csv_path, index=False)\n",
    "            print(f\"‚úÖ Converted to CSV: {csv_path}\")\n",
    "            return df, csv_path\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error reading Excel file: {e}\")\n",
    "            return None, None\n",
    "    elif file_path.endswith('.gsheet'):\n",
    "        print(f\"üìÑ Detected Google Sheet: {file_path}\")\n",
    "        try:\n",
    "            # Colab-specific auth\n",
    "            from google.colab import auth\n",
    "            auth.authenticate_user()\n",
    "            import gspread\n",
    "            from google.auth import default\n",
    "            \n",
    "            creds, _ = default()\n",
    "            gc = gspread.authorize(creds)\n",
    "            \n",
    "            # Extract filename without path and extension\n",
    "            filename = os.path.basename(file_path).replace('.gsheet', '')\n",
    "            print(f\"   Connecting to Google Sheet: '{filename}'...\")\n",
    "            \n",
    "            # Open by title\n",
    "            worksheet = gc.open(filename).sheet1\n",
    "            rows = worksheet.get_all_values()\n",
    "            \n",
    "            # Convert to DataFrame\n",
    "            df = pd.DataFrame(rows[1:], columns=rows[0])\n",
    "            \n",
    "            # Save as CSV for consistency\n",
    "            csv_path = file_path.replace('.gsheet', '.csv')\n",
    "            df.to_csv(csv_path, index=False)\n",
    "            print(f\"‚úÖ Converted to CSV: {csv_path}\")\n",
    "            return df, csv_path\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error reading Google Sheet. Ensure you are authenticated and the sheet name matches exactly.\")\n",
    "            print(f\"Error: {e}\")\n",
    "            return None, None\n",
    "    elif file_path.endswith('.csv'):\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            return df, file_path\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error reading CSV file: {e}\")\n",
    "            return None, None\n",
    "    else:\n",
    "        print(f\"‚ùå Unsupported file format: {file_path}\")\n",
    "        return None, None\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. IMPORTS & CONFIG\n",
    "# ==============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"‚öôÔ∏è Using Device: {DEVICE}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. JIRA FETCHER (Extract Data)\n",
    "# ==============================================================================\n",
    "class JiraFetcher:\n",
    "    def __init__(self):\n",
    "        self.session = requests.Session()\n",
    "        self.session.auth = (JIRA_EMAIL, JIRA_API_TOKEN)\n",
    "        self.session.headers.update({'Content-Type': 'application/json', 'Accept': 'application/json'})\n",
    "\n",
    "    def get_ticket_details(self, issue_key):\n",
    "        url = f\"{JIRA_URL}/rest/api/3/issue/{issue_key}\"\n",
    "        params = {'fields': 'summary,comment', 'expand': 'renderedFields'}\n",
    "        \n",
    "        retries = 0\n",
    "        max_retries = 5\n",
    "        base_wait = 5\n",
    "        \n",
    "        while retries <= max_retries:\n",
    "            try:\n",
    "                response = self.session.get(url, params=params, timeout=30)\n",
    "                if response.status_code == 200:\n",
    "                    return response.json()\n",
    "                elif response.status_code == 429:\n",
    "                    wait_time = base_wait * (2 ** retries)\n",
    "                    print(f\"‚ö†Ô∏è Rate limited on {issue_key}. Retrying in {wait_time}s...\")\n",
    "                    time.sleep(wait_time)\n",
    "                    retries += 1\n",
    "                else:\n",
    "                    print(f\"‚ùå Failed to fetch {issue_key}: Status {response.status_code}\")\n",
    "                    return None\n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching {issue_key}: {e}\")\n",
    "                return None\n",
    "        \n",
    "        print(f\"‚ùå Max retries reached for {issue_key}\")\n",
    "        return None\n",
    "\n",
    "    def extract_conversation(self, issue_data):\n",
    "        if not issue_data: return \"\"\n",
    "        fields = issue_data.get('fields', {})\n",
    "        text = fields.get('summary', '') + \" \"\n",
    "        comments = fields.get('comment', {}).get('comments', [])\n",
    "        for comment in comments:\n",
    "            body = comment.get('body', {})\n",
    "            text += self._extract_text(body) + \" \"\n",
    "        return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    def _extract_text(self, body):\n",
    "        text_parts = []\n",
    "        if isinstance(body, dict) and 'content' in body:\n",
    "            for node in body['content']:\n",
    "                if 'content' in node:\n",
    "                    for text_node in node['content']:\n",
    "                        if 'text' in text_node:\n",
    "                            text_parts.append(text_node['text'])\n",
    "        return \" \".join(text_parts)\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. HEURISTIC LABELING (Cold Start)\n",
    "# ==============================================================================\n",
    "def get_heuristic_label(text):\n",
    "    text = str(text).lower()\n",
    "    positive = [\"thank\", \"resolved\", \"fixed\", \"appreciate\", \"great\", \"good\", \"working now\"]\n",
    "    negative = [\"not working\", \"fail\", \"error\", \"broken\", \"bad\", \"terrible\", \"frustrated\", \"delay\"]\n",
    "    \n",
    "    if any(kw in text for kw in [\"working now\", \"resolved\", \"fixed\"]) and not any(kw in text for kw in [\"not resolved\", \"not fixed\"]):\n",
    "        return \"Positive\"\n",
    "    if any(kw in text for kw in positive): return \"Positive\"\n",
    "    if any(kw in text for kw in negative): return \"Negative\"\n",
    "    return \"Neutral\"\n",
    "\n",
    "def create_training_data(file_path):\n",
    "    print(f\"üè∑Ô∏è Creating training data from {file_path}...\")\n",
    "    df, csv_path = load_data(file_path)\n",
    "    if df is None: return None\n",
    "    \n",
    "    # 1. Fetch Text if not present\n",
    "    if 'text' not in df.columns:\n",
    "        print(\"   Fetching ticket text from Jira...\")\n",
    "        jira = JiraFetcher()\n",
    "        texts = []\n",
    "        try:\n",
    "            keys_col = 'Key' if 'Key' in df.columns else df.columns[0]\n",
    "            # Limit for demo/speed check\n",
    "            keys = df[keys_col].tolist()\n",
    "            \n",
    "            # Sequential processing to strictly avoid rate limits\n",
    "            print(f\"   (Switching to sequential fetching to avoid 429 errors. Total: {len(keys)})\")\n",
    "            \n",
    "            success_count = 0\n",
    "            for i, key in enumerate(keys):\n",
    "                try:\n",
    "                    # Log every 10 tickets or important status\n",
    "                    if i % 5 == 0: \n",
    "                        print(f\"   ‚è≥ Processing {i+1}/{len(keys)}: {key}...\", end='\\r')\n",
    "                        \n",
    "                    data = jira.get_ticket_details(key)\n",
    "                    if data:\n",
    "                        # Extract Summary + Comments\n",
    "                        txt = jira.extract_conversation(data)\n",
    "                        texts.append({'key': key, 'text': txt})\n",
    "                        success_count += 1\n",
    "                    \n",
    "                    # Consistent delay between every request\n",
    "                    time.sleep(1.0) \n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ùå Error on {key}: {e}\")\n",
    "            \n",
    "            print(f\"\\n   ‚úÖ Finished fetching. Success: {success_count}/{len(keys)}\")\n",
    "            \n",
    "            text_df = pd.DataFrame(texts)\n",
    "            df = df.merge(text_df, left_on=keys_col, right_on='key', how='left')\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error fetching data: {e}\")\n",
    "            return None\n",
    "\n",
    "    # 2. Apply Heuristics\n",
    "    print(\"üß† Applying Heuristic Rules (Keyword Logic)...\")\n",
    "    df['label'] = df['text'].apply(get_heuristic_label)\n",
    "    output_path = os.path.join(DATA_DIR, 'labeled_training_data.csv')\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"\\n‚úÖ Saved labeled data to {output_path}\")\n",
    "    print(df['label'].value_counts())\n",
    "    return output_path\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. BERT TRAINING\n",
    "# ==============================================================================\n",
    "class SatisfactionDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self): return len(self.texts)\n",
    "    def __getitem__(self, item):\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            str(self.texts[item]), max_length=self.max_len, padding='max_length',\n",
    "            truncation=True, return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(self.labels[item], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def train_bert(data_path):\n",
    "    print(\"\\nüèãÔ∏è Starting BERT Training...\")\n",
    "    # Keep default na values but convert empty text to NaN so we can drop it\n",
    "    df = pd.read_csv(data_path)\n",
    "    df = df.dropna(subset=['text', 'label'])\n",
    "    \n",
    "    if len(df) == 0:\n",
    "        print(\"‚ùå Error: Dataset is empty after filtering! Text fetching likely failed.\")\n",
    "        print(\"   Check your JIRA_URL/EMAIL/TOKEN secrets and try again.\")\n",
    "        return\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    df['label_encoded'] = le.fit_transform(df['label'])\n",
    "    joblib.dump(le, os.path.join(MODELS_DIR, 'label_encoder.joblib'))\n",
    "    \n",
    "    train_df, val_df = train_test_split(df, test_size=0.2)\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    train_ds = SatisfactionDataset(train_df.text.values, train_df.label_encoded.values, tokenizer, 128)\n",
    "    val_ds = SatisfactionDataset(val_df.text.values, val_df.label_encoded.values, tokenizer, 128)\n",
    "    \n",
    "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(le.classes_))\n",
    "    model = model.to(DEVICE)\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "    train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "    \n",
    "    print(f\"   Training on {len(train_df)} samples, Validating on {len(val_df)} samples.\")\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(3):\n",
    "        total_loss = 0\n",
    "        print(f\"   Epoch {epoch+1}/3...\", end=' ')\n",
    "        batch_count = 0\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            mask = batch['attention_mask'].to(DEVICE)\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask=mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            batch_count += 1\n",
    "            \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "    model.save_pretrained(MODELS_DIR)\n",
    "    tokenizer.save_pretrained(MODELS_DIR)\n",
    "    print(\"‚úÖ Model trained and saved!\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 6. PREDICTION ON FULL SHEET\n",
    "# ==============================================================================\n",
    "def predict_full_sheet(file_path):\n",
    "    print(f\"\\nüîÆ Generating Predictions for {file_path}...\")\n",
    "    tokenizer = BertTokenizer.from_pretrained(MODELS_DIR)\n",
    "    model = BertForSequenceClassification.from_pretrained(MODELS_DIR)\n",
    "    model = model.to(DEVICE)\n",
    "    model.eval()\n",
    "    le = joblib.load(os.path.join(MODELS_DIR, 'label_encoder.joblib'))\n",
    "    \n",
    "    df, csv_path = load_data(file_path)\n",
    "    if df is None: return\n",
    "\n",
    "    jira = JiraFetcher()\n",
    "    \n",
    "    # Needs text column first\n",
    "    if 'text' not in df.columns:\n",
    "        print(\"   Fetching text for prediction (this might take time)...\")\n",
    "        # Logic to fetch text would go here similar to create_training_data\n",
    "        # For brevity assuming create_training_data was run or we fetch on fly\n",
    "        # (Reusing fetch logic needed if new file)\n",
    "    \n",
    "    # Prediction Loop\n",
    "    predictions = []\n",
    "    confidences = []\n",
    "    \n",
    "    # Process batching for speed\n",
    "    texts = df['text'].tolist() if 'text' in df.columns else [] # Placeholder\n",
    "    \n",
    "    # If text is missing, we must fetch it. \n",
    "    # For independent running, we assume the user provides a CSV that needs processing.\n",
    "    # We will implement a simple sequential fetch-predict for robustness in Colab\n",
    "    \n",
    "    keys_col = 'Key' if 'Key' in df.columns else df.columns[0]\n",
    "    keys = df[keys_col].tolist()\n",
    "    \n",
    "    for i, key in enumerate(keys):\n",
    "        try:\n",
    "            # Check if text exists, else fetch\n",
    "            text = df.loc[i, 'text'] if 'text' in df.columns else None\n",
    "            if not isinstance(text, str):\n",
    "                details = jira.get_ticket_details(key)\n",
    "                text = jira.extract_conversation(details)\n",
    "            \n",
    "            inputs = tokenizer(str(text), return_tensors='pt', max_length=128, truncation=True, padding='max_length')\n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs['input_ids'].to(DEVICE), attention_mask=inputs['attention_mask'].to(DEVICE))\n",
    "                probs = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "                conf, pred_idx = torch.max(probs, dim=1)\n",
    "            \n",
    "            pred_label = le.inverse_transform([pred_idx.item()])[0]\n",
    "            predictions.append(pred_label)\n",
    "            confidences.append(conf.item())\n",
    "            \n",
    "            if i % 10 == 0: print(f\"   Processed {i}/{len(keys)}\", end='\\r')\n",
    "            time.sleep(0.5) # Proactive rate limiting\n",
    "            \n",
    "        except Exception as e:\n",
    "            predictions.append(\"Error\")\n",
    "            confidences.append(0.0)\n",
    "            \n",
    "    df['Predicted_Satisfaction'] = predictions\n",
    "    df['Confidence'] = confidences\n",
    "    \n",
    "    out_path = csv_path.replace('.csv', '_bert_colab_predictions.csv')\n",
    "    df.to_csv(out_path, index=False)\n",
    "    print(f\"\\n‚úÖ Predictions saved to {out_path}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 7. EXECUTION BLOCK\n",
    "# ==============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    # Example Workflow\n",
    "    print(\"Select an action:\")\n",
    "    print(\"1. Label Data & Train Model\")\n",
    "    print(\"2. Predict on New CSV\")\n",
    "    \n",
    "    # In Colab, you can just change this variable manually\n",
    "    ACTION = \"2\" \n",
    "    \n",
    "    if 'google.colab' in sys.modules:\n",
    "        print(f\"üìÇ Files in {DATA_DIR}:\")\n",
    "        try:\n",
    "            files = os.listdir(DATA_DIR)\n",
    "            for f in files: print(f\"   - {f}\")\n",
    "        except:\n",
    "            print(f\"   (Could not list files in {DATA_DIR})\")\n",
    "    \n",
    "    INPUT_CSV = os.path.join(DATA_DIR, \"Sheet for 3_months - Sheet1.csv\") # Default\n",
    "    \n",
    "    # Auto-detect if default missing\n",
    "    if not os.path.exists(INPUT_CSV) and os.path.exists(DATA_DIR):\n",
    "        print(f\"‚ö†Ô∏è Default file {INPUT_CSV} not found.\")\n",
    "        try:\n",
    "            # Find any file containing 'Sheet for 3_months' or just pick the first .gsheet/.csv\n",
    "            candidates = [f for f in os.listdir(DATA_DIR) if (f.endswith('.csv') or f.endswith('.gsheet')) and 'predictions' not in f and 'labeled' not in f]\n",
    "            if candidates:\n",
    "                # Prefer one with \"Sheet for 3_months\"\n",
    "                better_candidates = [f for f in candidates if \"Sheet for 3_months\" in f]\n",
    "                if better_candidates:\n",
    "                    chosen = better_candidates[0]\n",
    "                else:\n",
    "                    chosen = candidates[0]\n",
    "                    \n",
    "                INPUT_CSV = os.path.join(DATA_DIR, chosen)\n",
    "                print(f\"‚úÖ Auto-selected valid file: {INPUT_CSV}\")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # Allow user to override input filename easily\n",
    "    print(f\"\\nüìù Using Input File: {INPUT_CSV}\")\n",
    "    print(f\"   (If this is wrong, edit the INPUT_CSV variable in the code)\")\n",
    "    \n",
    "    if ACTION == \"1\":\n",
    "        labeled_path = create_training_data(INPUT_CSV)\n",
    "        if labeled_path:\n",
    "            train_bert(labeled_path)\n",
    "            predict_full_sheet(INPUT_CSV) # Predict on the same sheet after training\n",
    "            \n",
    "    elif ACTION == \"2\":\n",
    "        predict_full_sheet(INPUT_CSV)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
