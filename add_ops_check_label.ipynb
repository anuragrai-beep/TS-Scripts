{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Jira Operations Ticket Label Automation\n",
        "\n",
        "Automation to add 'potential_TS_issue' label to Jira Operations Tickets.\n",
        "\n",
        "**Features:**\n",
        "*   Uses fuzzy matching with keywords.\n",
        "*   Tracks processed tickets in CSV to avoid rescanning.\n",
        "*   Excludes tickets with 'Done' status.\n",
        "*   Excludes tickets with 'TS_SC' or 'script_cleared' labels.\n",
        "*   Uses date-based scanning.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import json\n",
        "import time\n",
        "import glob\n",
        "import csv\n",
        "from typing import List, Dict, Optional, Set\n",
        "from datetime import datetime, timedelta\n",
        "try:\n",
        "    from fuzzywuzzy import fuzz, process\n",
        "except ImportError:\n",
        "    # Install if missing (common in Colab)\n",
        "    !pip install fuzzywuzzy\n",
        "    !pip install python-Levenshtein\n",
        "    from fuzzywuzzy import fuzz, process\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# CONFIGURATION\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "# Set these using Colab Secrets or replace with your actual values if running locally\n",
        "JIRA_URL = \"https://certifyos.atlassian.net\"\n",
        "JIRA_EMAIL = os.getenv(\"JIRA_EMAIL\", \"anura@certifyos.com\")\n",
        "JIRA_API_TOKEN = os.getenv(\"JIRA_API_TOKEN\", \"ATATT3xFfGF0\")\n",
        "\n",
        "# CSV file to track processed tickets\n",
        "PROCESSED_TICKETS_CSV = \"processed_tickets.csv\"\n",
        "PROGRESS_TRACKER_CSV = \"scan_progress.csv\"\n",
        "\n",
        "# Default start date - November 21, 2025\n",
        "DEFAULT_START_DATE = \"2025-11-21\"\n",
        "\n",
        "POTENTIAL_TS_LABEL = \"potential_TS_issue\"\n",
        "FUZZY_MATCH_THRESHOLD = 80  # Minimum fuzzy match score (0-100)\n",
        "BATCH_SIZE = 1000  # Number of tickets to process per run\n",
        "\n",
        "# Common keywords/phrases to detect\n",
        "KEYWORDS = [\n",
        "    \"cannot submit\",\n",
        "    \"can't submit\",\n",
        "    \"unable to submit\",\n",
        "    \"Finish button\",\n",
        "    \"Finish and Submit\",\n",
        "    \"stuck on finish\",\n",
        "    \"spinning circle\",\n",
        "    \"not moving forward\",\n",
        "    \"signature screen\",\n",
        "    \"attestation\",\n",
        "    \"ready to sign\",\n",
        "    \"loop of finish edit\",\n",
        "    \"missing attestation\",\n",
        "    \"not loading\",\n",
        "    \"page not loading\",\n",
        "    \"form error\",\n",
        "    \"form submission error\",\n",
        "    \"glitch\",\n",
        "    \"expired license\",\n",
        "    \"missing PLI\",\n",
        "    \"update provider details\",\n",
        "    \"update email\",\n",
        "    \"wrong spelling\",\n",
        "    \"correct name\",\n",
        "    \"backfill\",\n",
        "    \"provider not credentialed\",\n",
        "    \"upload forms\",\n",
        "    \"SFTP\",\n",
        "    \"API\",\n",
        "    \"integration\",\n",
        "    \"firewall\",\n",
        "    \"endpoint\",\n",
        "    \"generate credentials\",\n",
        "    \"workflow timelines\",\n",
        "    \"automation failing\",\n",
        "    \"backend\",\n",
        "    \"null value\",\n",
        "    \"missing field\",\n",
        "    \"business rule\",\n",
        "    \"delete facility/group\",\n",
        "    \"bulk update\",\n",
        "    \"bug\",\n",
        "    \"platform issue\",\n",
        "    \"can't login\",\n",
        "    \"link expired\",\n",
        "    \"resend link\",\n",
        "    \"verification code not working\",\n",
        "    \"workflow audits\",\n",
        "    \"API not working\",\n",
        "    \"How to submit\",\n",
        "    \"Review error\",\n",
        "    \"why blocked\",\n",
        "    \"how to upload forms\",\n",
        "    \"Wrong name\",\n",
        "    \"update emai\",\n",
        "    \"expired license incorrect\",\n",
        "    \"missing data\",\n",
        "    \"GET DATA error\",\n",
        "    \"PSV Generation errored\",\n",
        "    \"fields missing\",\n",
        "    \"browser issues\",\n",
        "    \"Delete group\",\n",
        "    \"update group\",\n",
        "    \"bulk changes\",\n",
        "    \"Issue with Portal\"\n",
        "]\n",
        "\n",
        "# Ignore patterns - tickets containing BOTH strings in a pair will be ignored\n",
        "IGNORE_PATTERNS = [\n",
        "    (\"outreach@certifyos.com\", \"Action Required: \"),\n",
        "    (\"Notice of Canceled\", \"outreach@certifyos.com\"),\n",
        "    (\"Notice of Canceled\", \"credentialing@certifyos.com\"),\n",
        "    (\"credentialing@hioscar.com\", \"Virtru Secure Email\"),\n",
        "    (\"rosters@hioscar.com\", \"Virtru Secure Email\")\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class OpsTicketLabelAutomation:\n",
        "    \"\"\"Automation to add potential_TS_issue label to Operations Tickets based on fuzzy keyword matching\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize Jira session and load processed tickets\"\"\"\n",
        "        self.session = requests.Session()\n",
        "        self.session.auth = (JIRA_EMAIL, JIRA_API_TOKEN)\n",
        "        self.session.headers.update({\n",
        "            'Content-Type': 'application/json',\n",
        "            'Accept': 'application/json'\n",
        "        })\n",
        "        \n",
        "        # Load already processed tickets from CSV\n",
        "        self.processed_tickets: Set[str] = self.load_processed_tickets()\n",
        "        \n",
        "        # Load scan progress\n",
        "        self.scan_progress = self.load_scan_progress()\n",
        "        \n",
        "        # Normalize keywords for case-insensitive matching\n",
        "        self.keywords_lower = [kw.lower() for kw in KEYWORDS]\n",
        "    \n",
        "    def should_ignore_ticket(self, description: str, reporter_email: str = None) -> bool:\n",
        "        \"\"\"Check if ticket should be ignored based on IGNORE_PATTERNS or reporter\"\"\"\n",
        "        # Check reporter\n",
        "        if reporter_email and reporter_email.lower() == 'support@sendgrid.com':\n",
        "            return True\n",
        "\n",
        "        if not description:\n",
        "            return False\n",
        "        \n",
        "        desc_lower = description.lower()\n",
        "        \n",
        "        # Check each ignore pattern pair\n",
        "        for pattern1, pattern2 in IGNORE_PATTERNS:\n",
        "            if pattern1.lower() in desc_lower and pattern2.lower() in desc_lower:\n",
        "                return True\n",
        "        \n",
        "        return False\n",
        "    \n",
        "    def load_processed_tickets(self) -> Set[str]:\n",
        "        \"\"\"Load already processed ticket keys from CSV file\"\"\"\n",
        "        processed_tickets = set()\n",
        "        \n",
        "        if os.path.exists(PROCESSED_TICKETS_CSV):\n",
        "            try:\n",
        "                with open(PROCESSED_TICKETS_CSV, 'r', newline='', encoding='utf-8') as csvfile:\n",
        "                    reader = csv.reader(csvfile)\n",
        "                    for row in reader:\n",
        "                        if row:  # Skip empty rows\n",
        "                            processed_tickets.add(row[0].strip())\n",
        "                print(f\"   üìã Loaded {len(processed_tickets)} previously processed tickets from {PROCESSED_TICKETS_CSV}\")\n",
        "            except Exception as e:\n",
        "                print(f\"   ‚ö†Ô∏è  Error loading processed tickets CSV: {e}\")\n",
        "                # Create a new CSV file if it's corrupted\n",
        "                open(PROCESSED_TICKETS_CSV, 'w').close()\n",
        "        else:\n",
        "            print(f\"   üìã No existing processed tickets CSV found. Creating new one: {PROCESSED_TICKETS_CSV}\")\n",
        "            # Create CSV file with header\n",
        "            with open(PROCESSED_TICKETS_CSV, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "                writer = csv.writer(csvfile)\n",
        "                writer.writerow(['ticket_key', 'processed_date', 'matched_keyword', 'label_added'])\n",
        "        \n",
        "        return processed_tickets\n",
        "    \n",
        "    def load_scan_progress(self) -> Dict:\n",
        "        \"\"\"Load scan progress from tracker file\"\"\"\n",
        "        progress = {\n",
        "            'last_processed_date': DEFAULT_START_DATE,\n",
        "            'last_processed_ticket': None,\n",
        "            'last_scan_date': None,\n",
        "            'total_processed': 0,\n",
        "            'current_batch': 0\n",
        "        }\n",
        "        \n",
        "        if os.path.exists(PROGRESS_TRACKER_CSV):\n",
        "            try:\n",
        "                with open(PROGRESS_TRACKER_CSV, 'r', newline='', encoding='utf-8') as csvfile:\n",
        "                    reader = csv.DictReader(csvfile)\n",
        "                    for row in reader:\n",
        "                        progress = {\n",
        "                            'last_processed_date': row.get('last_processed_date', DEFAULT_START_DATE),\n",
        "                            'last_processed_ticket': row.get('last_processed_ticket'),\n",
        "                            'last_scan_date': row.get('last_scan_date'),\n",
        "                            'total_processed': int(row.get('total_processed', 0)),\n",
        "                            'current_batch': int(row.get('current_batch', 0)) + 1\n",
        "                        }\n",
        "                        break\n",
        "                print(f\"   üìä Loaded scan progress: {progress['total_processed']} tickets processed, last date: {progress['last_processed_date']}\")\n",
        "            except Exception as e:\n",
        "                print(f\"   ‚ö†Ô∏è  Error loading progress tracker: {e}\")\n",
        "                # Create a new progress tracker\n",
        "                self.save_scan_progress(progress)\n",
        "        else:\n",
        "            print(f\"   üìä No existing progress tracker found. Creating new one: {PROGRESS_TRACKER_CSV}\")\n",
        "            print(f\"   üìÖ Default start date: {DEFAULT_START_DATE}\")\n",
        "            self.save_scan_progress(progress)\n",
        "        \n",
        "        return progress\n",
        "    \n",
        "    def save_scan_progress(self, progress: Dict):\n",
        "        \"\"\"Save scan progress to tracker file\"\"\"\n",
        "        try:\n",
        "            with open(PROGRESS_TRACKER_CSV, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "                fieldnames = ['last_processed_date', 'last_processed_ticket', 'last_scan_date', 'total_processed', 'current_batch']\n",
        "                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "                writer.writeheader()\n",
        "                writer.writerow(progress)\n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ùå Error saving scan progress: {e}\")\n",
        "    \n",
        "    def save_processed_ticket(self, ticket_key: str, matched_keyword: str = \"\", label_added: bool = False):\n",
        "        \"\"\"Save processed ticket to CSV file\"\"\"\n",
        "        try:\n",
        "            with open(PROCESSED_TICKETS_CSV, 'a', newline='', encoding='utf-8') as csvfile:\n",
        "                writer = csv.writer(csvfile)\n",
        "                writer.writerow([\n",
        "                    ticket_key, \n",
        "                    datetime.now().isoformat(),\n",
        "                    matched_keyword,\n",
        "                    str(label_added)  \n",
        "                ])\n",
        "            self.processed_tickets.add(ticket_key)\n",
        "            \n",
        "            # Update progress\n",
        "            self.scan_progress['last_processed_ticket'] = ticket_key\n",
        "            self.scan_progress['last_scan_date'] = datetime.now().isoformat()\n",
        "            self.scan_progress['total_processed'] += 1\n",
        "            self.save_scan_progress(self.scan_progress)\n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ùå Error saving processed ticket to CSV: {e}\")\n",
        "    \n",
        "    def get_current_date_range(self) -> tuple:\n",
        "        \"\"\"Get the current date range for scanning\"\"\"\n",
        "        # Start from the last processed date or default start date\n",
        "        start_date = self.scan_progress['last_processed_date']\n",
        "        \n",
        "        # End date is today\n",
        "        end_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "        \n",
        "        return start_date, end_date\n",
        "    \n",
        "    def format_jira_date(self, date_str: str) -> str:\n",
        "        \"\"\"Format date for JQL query\"\"\"\n",
        "        try:\n",
        "            # Parse the date and format for JQL\n",
        "            date_obj = datetime.strptime(date_str, \"%Y-%m-%d\")\n",
        "            return date_obj.strftime(\"%Y-%m-%d\")\n",
        "        except ValueError:\n",
        "            # If already in correct format, return as is\n",
        "            return date_str\n",
        "    \n",
        "    def _extract_text_from_jira_content(self, content: Dict) -> str:\n",
        "        \"\"\"Extract plain text from Jira's structured content format\"\"\"\n",
        "        if not isinstance(content, dict):\n",
        "            return str(content) if content else ''\n",
        "        \n",
        "        text_parts = []\n",
        "        content_list = content.get('content', [])\n",
        "        \n",
        "        if isinstance(content_list, list):\n",
        "            for item in content_list:\n",
        "                if isinstance(item, dict):\n",
        "                    # Check for text directly\n",
        "                    text = item.get('text', '')\n",
        "                    if text:\n",
        "                        text_parts.append(text)\n",
        "                    # Check for nested content\n",
        "                    nested = item.get('content', [])\n",
        "                    if isinstance(nested, list):\n",
        "                        for nested_item in nested:\n",
        "                            if isinstance(nested_item, dict):\n",
        "                                nested_text = nested_item.get('text', '')\n",
        "                                if nested_text:\n",
        "                                    text_parts.append(nested_text)\n",
        "        \n",
        "        return ' '.join(text_parts)\n",
        "    \n",
        "    def extract_ticket_text(self, issue: Dict) -> str:\n",
        "        \"\"\"Extract and combine text from description, summary, and Title fields\"\"\"\n",
        "        fields = issue.get('fields', {})\n",
        "        \n",
        "        # Extract summary (standard field)\n",
        "        summary = fields.get('summary', '') or ''\n",
        "        \n",
        "        # Extract description (handle structured content)\n",
        "        description = fields.get('description', '')\n",
        "        if isinstance(description, dict):\n",
        "            desc_text = self._extract_text_from_jira_content(description)\n",
        "        else:\n",
        "            desc_text = str(description) if description else ''\n",
        "        \n",
        "        # Extract Title (customfield_12046)\n",
        "        title = fields.get('customfield_12046', '') or ''\n",
        "        if isinstance(title, dict):\n",
        "            title_text = self._extract_text_from_jira_content(title)\n",
        "        else:\n",
        "            title_text = str(title) if title else ''\n",
        "        \n",
        "        # Combine all text\n",
        "        combined_text = f\"{summary} {desc_text} {title_text}\"\n",
        "        \n",
        "        return combined_text\n",
        "    \n",
        "    def fuzzy_check_keywords_match(self, text: str) -> Optional[str]:\n",
        "        \"\"\"Check if any keyword fuzzy matches in the text. Returns matched keyword or None\"\"\"\n",
        "        text_lower = text.lower()\n",
        "        \n",
        "        # First check for exact matches (faster)\n",
        "        for keyword in self.keywords_lower:\n",
        "            if keyword in text_lower:\n",
        "                return keyword\n",
        "        \n",
        "        # If no exact match, try fuzzy matching\n",
        "        for keyword in self.keywords_lower:\n",
        "            # Use partial ratio for substring matching\n",
        "            score = fuzz.partial_ratio(keyword, text_lower)\n",
        "            if score >= FUZZY_MATCH_THRESHOLD:\n",
        "                print(f\"      üîç Fuzzy match: '{keyword}' (score: {score})\")\n",
        "                return keyword\n",
        "        \n",
        "        return None\n",
        "    \n",
        "    def get_issue_labels(self, issue_key: str) -> List[str]:\n",
        "        \"\"\"Get current labels for an issue\"\"\"\n",
        "        url = f\"{JIRA_URL}/rest/api/3/issue/{issue_key}?fields=labels\"\n",
        "        try:\n",
        "            response = self.session.get(url, timeout=30)\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                return data.get('fields', {}).get('labels', [])\n",
        "            return []\n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ö†Ô∏è  Error getting labels for {issue_key}: {e}\")\n",
        "            return []\n",
        "    \n",
        "    def add_potential_ts_label(self, issue_key: str) -> bool:\n",
        "        \"\"\"Add potential_TS_issue label to an issue without removing existing labels\"\"\"\n",
        "        # Get current labels\n",
        "        current_labels = self.get_issue_labels(issue_key)\n",
        "        \n",
        "        # Check if label already exists\n",
        "        if POTENTIAL_TS_LABEL in current_labels:\n",
        "            return True  # Already labeled\n",
        "        \n",
        "        # Add the new label to existing labels\n",
        "        updated_labels = current_labels + [POTENTIAL_TS_LABEL]\n",
        "        \n",
        "        # Update the issue\n",
        "        url = f\"{JIRA_URL}/rest/api/3/issue/{issue_key}\"\n",
        "        payload = {\n",
        "            \"fields\": {\n",
        "                \"labels\": updated_labels\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        try:\n",
        "            response = self.session.put(url, json=payload, timeout=30)\n",
        "            if response.status_code == 204:\n",
        "                return True\n",
        "            else:\n",
        "                print(f\"   ‚ùå Failed to add potential_TS_issue label to {issue_key}: {response.status_code} - {response.text[:200]}\")\n",
        "                return False\n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ùå Error adding potential_TS_issue label to {issue_key}: {e}\")\n",
        "            return False\n",
        "    \n",
        "    def find_tickets_to_process(self, batch_size: int = BATCH_SIZE) -> List[Dict]:\n",
        "        \"\"\"Find tickets that need processing using date-based scanning\"\"\"\n",
        "        print(\"=\" * 80)\n",
        "        print(\"üîç DATE-BASED TICKET SCANNING\")\n",
        "        print(\"=\" * 80)\n",
        "        \n",
        "        # Get current date range\n",
        "        start_date, end_date = self.get_current_date_range()\n",
        "        formatted_start = self.format_jira_date(start_date)\n",
        "        formatted_end = self.format_jira_date(end_date)\n",
        "        \n",
        "        print(f\"   üìÖ Scanning date range: {formatted_start} to {formatted_end}\")\n",
        "        print(f\"   üì¶ Batch size: {batch_size} tickets\")\n",
        "        \n",
        "        # Build JQL query - exclude Done status tickets and use date range\n",
        "        jql = (\n",
        "            'project = TS AND '\n",
        "            'issuetype = \"Operations Ticket\" AND '\n",
        "            '(labels IN (\"Credentialing_Inbox\", \"outreach_inbox\") OR '\n",
        "            '\"Request Type\" IN (\"Outreach Inbox Emailed request (TS)\", \"Credentialing Inbox Emailed request (TS)\")) AND '\n",
        "            'status != Done AND '\n",
        "            'labels != \"script_cleared\" AND '\n",
        "            'labels != \"TS_SC\" AND '\n",
        "            f'created >= \"{formatted_start}\" AND created <= \"{formatted_end}\" '\n",
        "            'ORDER BY created ASC, key ASC'  # Process oldest first\n",
        "        )\n",
        "        \n",
        "        url = f\"{JIRA_URL}/rest/api/3/search/jql\"\n",
        "        start_at = 0\n",
        "        max_results = 100\n",
        "        \n",
        "        print(f\"\\n   üéØ JQL Query: {jql}\\n\")\n",
        "        \n",
        "        total_found = None\n",
        "        tickets_to_process = []\n",
        "        processed_count = 0\n",
        "        latest_ticket_date = start_date\n",
        "        \n",
        "        while True:\n",
        "            current_max = max_results\n",
        "            remaining_batch = batch_size - processed_count\n",
        "            if remaining_batch < max_results:\n",
        "                current_max = remaining_batch\n",
        "            \n",
        "            params = {\n",
        "                'jql': jql,\n",
        "                'startAt': start_at,\n",
        "                'maxResults': current_max,\n",
        "                'fields': 'summary,description,customfield_12046,labels,key,status,assignee,reporter,created'\n",
        "            }\n",
        "            \n",
        "            try:\n",
        "                response = self.session.get(url, params=params, timeout=60)\n",
        "                if response.status_code != 200:\n",
        "                    print(f\"\\n   ‚ùå Error searching tickets: {response.status_code} - {response.text[:200]}\")\n",
        "                    break\n",
        "                \n",
        "                data = response.json()\n",
        "                issues = data.get('issues', [])\n",
        "                \n",
        "                if total_found is None:\n",
        "                    total_found = data.get('total', len(issues))\n",
        "                \n",
        "                if not issues:\n",
        "                    break\n",
        "                \n",
        "                if total_found is not None and start_at == 0:\n",
        "                    print(f\"   üìä Found {total_found} tickets in date range {formatted_start} to {formatted_end}\")\n",
        "                \n",
        "                # Process each issue\n",
        "                for issue in issues:\n",
        "                    issue_key = issue.get('key')\n",
        "                    if not issue_key:\n",
        "                        continue\n",
        "                    \n",
        "                    # Skip if already processed (from previous runs)\n",
        "                    if issue_key in self.processed_tickets:\n",
        "                        processed_count += 1\n",
        "                        continue\n",
        "                    \n",
        "                    fields = issue.get('fields', {})\n",
        "                    created_date = fields.get('created', '')\n",
        "                    \n",
        "                    # Extract date from created timestamp (format: \"2024-01-01T10:00:00.000+0000\")\n",
        "                    if created_date:\n",
        "                        try:\n",
        "                            ticket_date = created_date.split('T')[0]  # Extract YYYY-MM-DD\n",
        "                            if ticket_date > latest_ticket_date:\n",
        "                                latest_ticket_date = ticket_date\n",
        "                        except:\n",
        "                            pass\n",
        "                    \n",
        "                    # Extract text from fields\n",
        "                    summary = fields.get('summary', '') or ''\n",
        "                    \n",
        "                    # Extract description\n",
        "                    description = fields.get('description', '')\n",
        "                    if isinstance(description, dict):\n",
        "                        desc_text = self._extract_text_from_jira_content(description)\n",
        "                    else:\n",
        "                        desc_text = str(description) if description else ''\n",
        "                    \n",
        "                    # Extract Title (customfield_12046)\n",
        "                    title = fields.get('customfield_12046', '') or ''\n",
        "                    if isinstance(title, dict):\n",
        "                        title_text = self._extract_text_from_jira_content(title)\n",
        "                    else:\n",
        "                        title_text = str(title) if title else ''\n",
        "                    \n",
        "                    # Get current labels\n",
        "                    current_labels = fields.get('labels', [])\n",
        "                    has_potential_ts_label = POTENTIAL_TS_LABEL in current_labels\n",
        "                    \n",
        "                    # Store ticket data\n",
        "                    ticket_data = {\n",
        "                        'issue_key': issue_key,\n",
        "                        'summary': summary,\n",
        "                        'description': desc_text,\n",
        "                        'title': title_text,\n",
        "                        'combined_text': f\"{summary} {desc_text} {title_text}\",\n",
        "                        'status': fields.get('status', {}).get('name', ''),\n",
        "                        'assignee': fields.get('assignee'),\n",
        "                        'reporter_email': fields.get('reporter', {}).get('emailAddress', '') if fields.get('reporter') else '',\n",
        "                        'has_potential_ts_label': has_potential_ts_label,\n",
        "                        'labels': current_labels,\n",
        "                        'created': created_date\n",
        "                    }\n",
        "                    \n",
        "                    tickets_to_process.append(ticket_data)\n",
        "                    processed_count += 1\n",
        "                    \n",
        "                    if processed_count >= batch_size:\n",
        "                        break\n",
        "                \n",
        "                # Update start_at for next page\n",
        "                start_at += len(issues)\n",
        "                \n",
        "                # Progress update\n",
        "                progress = f\"   ‚úÖ Checked {start_at} tickets | New to process: {len(tickets_to_process)}\"\n",
        "                if total_found:\n",
        "                    progress += f\" | Total in range: {total_found}\"\n",
        "                print(progress, end='\\r')\n",
        "                \n",
        "                if len(issues) < current_max or processed_count >= batch_size:\n",
        "                    break\n",
        "                \n",
        "                time.sleep(0.3)\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"\\n   ‚ùå Error fetching tickets at page {start_at}: {e}\")\n",
        "                start_at += max_results\n",
        "                time.sleep(1)\n",
        "                continue\n",
        "        \n",
        "        # Update progress with the latest date processed\n",
        "        if tickets_to_process:\n",
        "            self.scan_progress['last_processed_date'] = latest_ticket_date\n",
        "            self.save_scan_progress(self.scan_progress)\n",
        "        \n",
        "        print(f\"\\n\\nüìä Batch scan complete:\")\n",
        "        print(f\"   ‚Ä¢ Date range: {formatted_start} to {formatted_end}\")\n",
        "        print(f\"   ‚Ä¢ New tickets to process: {len(tickets_to_process)}\")\n",
        "        print(f\"   ‚Ä¢ Progress updated to date: {latest_ticket_date}\")\n",
        "        if total_found:\n",
        "            print(f\"   ‚Ä¢ Total tickets in this date range: {total_found}\")\n",
        "        \n",
        "        return tickets_to_process\n",
        "    \n",
        "    def process_tickets(self, dry_run: bool = False, batch_size: int = BATCH_SIZE) -> Dict:\n",
        "        \"\"\"Main processing function\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"üè∑Ô∏è  DATE-BASED TS ISSUE LABEL AUTOMATION\")\n",
        "        print(\"=\" * 80)\n",
        "        print(f\"   ‚Ä¢ Fuzzy match threshold: {FUZZY_MATCH_THRESHOLD}%\")\n",
        "        print(f\"   ‚Ä¢ Batch size: {batch_size} tickets per run\")\n",
        "        print(f\"   ‚Ä¢ Default start date: {DEFAULT_START_DATE}\")\n",
        "        print(f\"   ‚Ä¢ Tracking processed tickets in: {PROCESSED_TICKETS_CSV}\")\n",
        "        print(f\"   ‚Ä¢ Progress tracking in: {PROGRESS_TRACKER_CSV}\")\n",
        "        print(f\"   ‚Ä¢ Excluding tickets with 'Done' status\")\n",
        "        print(f\"   ‚Ä¢ Excluding tickets with 'TS_SC' or 'script_cleared' labels\")\n",
        "        print(f\"   ‚Ä¢ Current progress: {self.scan_progress['total_processed']} tickets processed\")\n",
        "        print(f\"   ‚Ä¢ Current batch: #{self.scan_progress['current_batch']}\")\n",
        "        print(f\"   ‚Ä¢ Last processed date: {self.scan_progress['last_processed_date']}\")\n",
        "        \n",
        "        if dry_run:\n",
        "            print(\"‚ö†Ô∏è  DRY RUN MODE - No labels will be added\\n\")\n",
        "        \n",
        "        # Find tickets that need processing using date-based scanning\n",
        "        tickets_to_process = self.find_tickets_to_process(batch_size=batch_size)\n",
        "        \n",
        "        if not tickets_to_process:\n",
        "            print(\"‚úÖ No new tickets to process in this batch\")\n",
        "            # Check if we've reached today's date\n",
        "            start_date, end_date = self.get_current_date_range()\n",
        "            if start_date >= end_date:\n",
        "                print(\"üéâ All tickets up to today have been processed!\")\n",
        "            return {\n",
        "                'processed': 0,\n",
        "                'labeled': 0,\n",
        "                'skipped_already_labeled': 0,\n",
        "                'skipped_no_match': 0,\n",
        "                'failed': 0\n",
        "            }\n",
        "        \n",
        "        # Process each ticket\n",
        "        labeled_count = 0\n",
        "        skipped_already_labeled = 0\n",
        "        skipped_no_match = 0\n",
        "        failed_count = 0\n",
        "        \n",
        "        print(f\"\\nüîç Processing {len(tickets_to_process)} new tickets in batch #{self.scan_progress['current_batch']}...\")\n",
        "        \n",
        "        for idx, ticket in enumerate(tickets_to_process, 1):\n",
        "            issue_key = ticket['issue_key']\n",
        "            combined_text = ticket['combined_text']\n",
        "            has_existing_label = ticket['has_potential_ts_label']\n",
        "            status = ticket['status']\n",
        "            created = ticket['created'][:10] if ticket['created'] else 'Unknown'\n",
        "            \n",
        "            print(f\"   [{idx}/{len(tickets_to_process)}] {issue_key} ({status}, {created}): \", end='', flush=True)\n",
        "            \n",
        "            # Check if ticket should be ignored based on description patterns or reporter\n",
        "            if self.should_ignore_ticket(ticket['description'], ticket.get('reporter_email')):\n",
        "                print(f\"üö´ Ignored (automated notification)\")\n",
        "                self.save_processed_ticket(issue_key, \"ignored_pattern\", False)\n",
        "                skipped_no_match += 1\n",
        "                continue\n",
        "            \n",
        "            # Skip if already has the label\n",
        "            if has_existing_label:\n",
        "                print(f\"‚è≠Ô∏è  Already labeled\")\n",
        "                self.save_processed_ticket(issue_key, \"already_labeled\", False)\n",
        "                skipped_already_labeled += 1\n",
        "                continue\n",
        "            \n",
        "            # Check for keyword matches using fuzzy matching\n",
        "            matched_keyword = self.fuzzy_check_keywords_match(combined_text)\n",
        "            \n",
        "            if not matched_keyword:\n",
        "                print(f\"‚ùå No keyword match\")\n",
        "                self.save_processed_ticket(issue_key, \"no_match\", False)\n",
        "                skipped_no_match += 1\n",
        "                continue\n",
        "            \n",
        "            # We have a match - add label\n",
        "            if dry_run:\n",
        "                print(f\"‚úÖ DRY RUN: Would label (matched: '{matched_keyword}')\")\n",
        "                self.save_processed_ticket(issue_key, matched_keyword, True)\n",
        "                labeled_count += 1\n",
        "            else:\n",
        "                print(f\"üè∑Ô∏è  Adding label (matched: '{matched_keyword}')...\", end='', flush=True)\n",
        "                success = self.add_potential_ts_label(issue_key)\n",
        "                if success:\n",
        "                    print(\" ‚úÖ\")\n",
        "                    self.save_processed_ticket(issue_key, matched_keyword, True)\n",
        "                    labeled_count += 1\n",
        "                else:\n",
        "                    print(\" ‚ùå Failed\")\n",
        "                    self.save_processed_ticket(issue_key, matched_keyword, False)\n",
        "                    failed_count += 1\n",
        "            \n",
        "            # Rate limiting\n",
        "            if not dry_run:\n",
        "                time.sleep(0.5)\n",
        "        \n",
        "        # Summary\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"üìã BATCH PROCESSING SUMMARY\")\n",
        "        print(\"=\" * 80)\n",
        "        print(f\"   ‚úÖ Newly labeled: {labeled_count}\")\n",
        "        print(f\"   ‚è≠Ô∏è  Skipped (already labeled): {skipped_already_labeled}\")\n",
        "        print(f\"   üîç Skipped (no keyword match): {skipped_no_match}\")\n",
        "        print(f\"   ‚ùå Failed: {failed_count}\")\n",
        "        print(f\"   üìä Total processed in this batch: {len(tickets_to_process)}\")\n",
        "        print(f\"   üíæ Total tracked in CSV: {len(self.processed_tickets)}\")\n",
        "        print(f\"   üìà Overall progress: {self.scan_progress['total_processed']} tickets\")\n",
        "        print(f\"   üìÖ Next batch will continue from: {self.scan_progress['last_processed_date']}\")\n",
        "        print(\"=\" * 80 + \"\\n\")\n",
        "        \n",
        "        return {\n",
        "            'processed': len(tickets_to_process),\n",
        "            'labeled': labeled_count,\n",
        "            'skipped_already_labeled': skipped_already_labeled,\n",
        "            'skipped_no_match': skipped_no_match,\n",
        "            'failed': failed_count\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ------------------------------------------------------------\n",
        "# MAIN EXECUTION\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "# Configuration variables (override these in the notebook as needed)\n",
        "DRY_RUN = False     # Set to True to test without adding labels\n",
        "RESET_PROGRESS = False # Set to True to start fresh from DEFAULT_START_DATE\n",
        "\n",
        "print(f\"\\n{'=' * 80}\")\n",
        "print(f\"üì¶ Batch size: {BATCH_SIZE} tickets\")\n",
        "print(f\"üìÖ Default start date: {DEFAULT_START_DATE}\")\n",
        "print(f\"üíæ Tracking processed tickets in: {PROCESSED_TICKETS_CSV}\")\n",
        "print(f\"üìä Progress tracking in: {PROGRESS_TRACKER_CSV}\")\n",
        "print(f\"üîç Using fuzzy matching with {len(KEYWORDS)} keywords\")\n",
        "print(f\"üö´ Excluding tickets with 'Done' status\")\n",
        "print(f\"üö´ Excluding tickets with 'TS_SC' or 'script_cleared' labels\")\n",
        "if RESET_PROGRESS:\n",
        "    print(f\"üîÑ RESET MODE - Progress will be reset to {DEFAULT_START_DATE}\")\n",
        "print(f\"{'=' * 80}\\n\")\n",
        "\n",
        "automation = OpsTicketLabelAutomation()\n",
        "\n",
        "# Reset progress if requested\n",
        "if RESET_PROGRESS:\n",
        "    print(\"üîÑ Resetting progress tracker...\")\n",
        "    automation.scan_progress = {\n",
        "        'last_processed_date': DEFAULT_START_DATE,\n",
        "        'last_processed_ticket': None,\n",
        "        'last_scan_date': None,\n",
        "        'total_processed': 0,\n",
        "        'current_batch': 0\n",
        "    }\n",
        "    automation.save_scan_progress(automation.scan_progress)\n",
        "    print(f\"‚úÖ Progress reset to start date: {DEFAULT_START_DATE}\\n\")\n",
        "\n",
        "results = automation.process_tickets(dry_run=DRY_RUN, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Scheduling in Google Colab\n",
        "\n",
        "Since Google Colab is an interactive environment, it doesn't have a built-in \"Cron Job\" scheduler like a server.\n",
        "\n",
        "However, you can achieve a scheduled interval run using a simple loop while keeping your browser tab open.\n",
        "\n",
        "### Method: The Keep-Alive Loop\n",
        "\n",
        "Run the cell below to verify tickets every X hours automatically (as long as the Colab tab is open).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "# How often to run (in minutes)\n",
        "INTERVAL_MINUTES = 60\n",
        "\n",
        "print(f\"üöÄ Starting continuous scheduler. Will run every {INTERVAL_MINUTES} minutes.\")\n",
        "\n",
        "while True:\n",
        "    print(f\"\\n‚è∞ Starting run at {datetime.now().isoformat()}...\")\n",
        "    \n",
        "    # Initialize and run\n",
        "    automation = OpsTicketLabelAutomation()\n",
        "    automation.process_tickets(dry_run=False, batch_size=BATCH_SIZE)\n",
        "    \n",
        "    print(f\"üí§ Sleeping for {INTERVAL_MINUTES} minutes...\")\n",
        "    time.sleep(INTERVAL_MINUTES * 60)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
